{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.9.1+cpu'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# logistic回归实战\n",
    "在这一章里面，我们将处理一下结构化数据，并使用logistic回归对结构化数据进行简单的分类。\n",
    "## logistic回归介绍\n",
    "logistic回归是一种广义线性回归（generalized linear model），与多重线性回归分析有很多相同之处。它们的模型形式基本上相同，都具有 wx + b，其中w和b是待求参数，其区别在于他们的因变量不同，多重线性回归直接将wx+b作为因变量，即y =wx+b,而logistic回归则通过函数L将wx+b对应一个隐状态p，p =L(wx+b),然后根据p 与1-p的大小决定因变量的值。如果L是logistic函数，就是logistic回归，如果L是多项式函数就是多项式回归。\n",
    "\n",
    "说的更通俗一点，就是logistic回归会在线性回归后再加一层logistic函数的调用。\n",
    "\n",
    "logistic回归主要是进行二分类预测，我们在激活函数时候讲到过 Sigmod函数，Sigmod函数是最常见的logistic函数，因为Sigmod函数的输出的是是对于0~1之间的概率值，当概率大于0.5预测为1，小于0.5预测为0。\n",
    "\n",
    "下面我们就来使用公开的数据来进行介绍"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UCI German Credit  数据集\n",
    "\n",
    "UCI German Credit是UCI的德国信用数据集，里面有原数据和数值化后的数据。\n",
    "\n",
    "German Credit数据是根据个人的银行贷款信息和申请客户贷款逾期发生情况来预测贷款违约倾向的数据集，数据集包含24个维度的，1000条数据，\n",
    "\n",
    "在这里我们直接使用处理好的数值化的数据，作为展示。\n",
    "\n",
    "[地址](https://archive.ics.uci.edu/ml/machine-learning-databases/statlog/german/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 代码实战\n",
    "我们这里使用的 german.data-numeric是numpy处理好数值化数据，我们直接使用numpy的load方法读取即可"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=np.loadtxt(\"german.data-numeric\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "数据读取完成后我们要对数据做一下归一化的处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "n,l=data.shape\n",
    "for j in range(l-1):\n",
    "    meanVal=np.mean(data[:,j])\n",
    "    stdVal=np.std(data[:,j])\n",
    "    data[:,j]=(data[:,j]-meanVal)/stdVal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "打乱数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.shuffle(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "区分训练集和测试集，由于这里没有验证集，所以我们直接使用测试集的准确度作为评判好坏的标准\n",
    "\n",
    "区分规则：900条用于训练，100条作为测试\n",
    "\n",
    "german.data-numeric的格式为，前24列为24个维度，最后一个为要打的标签（0，1），所以我们将数据和标签一起区分出来"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_cnt = 900\n",
    "train_data=data[:train_cnt,:l-1]\n",
    "train_lab=data[:train_cnt,l-1]-1\n",
    "test_data=data[train_cnt:,:l-1]\n",
    "test_lab=data[train_cnt:,l-1]-1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "【TODO】定义模型，训练，测试"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用torch库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0/4000], Loss:0.6998\n",
      "Epoch [100/4000], Loss:0.6864\n",
      "Epoch [200/4000], Loss:0.6740\n",
      "Epoch [300/4000], Loss:0.6628\n",
      "Epoch [400/4000], Loss:0.6528\n",
      "Epoch [500/4000], Loss:0.6437\n",
      "Epoch [600/4000], Loss:0.6357\n",
      "Epoch [700/4000], Loss:0.6285\n",
      "Epoch [800/4000], Loss:0.6221\n",
      "Epoch [900/4000], Loss:0.6164\n",
      "Epoch [1000/4000], Loss:0.6113\n",
      "Epoch [1100/4000], Loss:0.6067\n",
      "Epoch [1200/4000], Loss:0.6025\n",
      "Epoch [1300/4000], Loss:0.5987\n",
      "Epoch [1400/4000], Loss:0.5953\n",
      "Epoch [1500/4000], Loss:0.5922\n",
      "Epoch [1600/4000], Loss:0.5893\n",
      "Epoch [1700/4000], Loss:0.5866\n",
      "Epoch [1800/4000], Loss:0.5842\n",
      "Epoch [1900/4000], Loss:0.5820\n",
      "Epoch [2000/4000], Loss:0.5799\n",
      "Epoch [2100/4000], Loss:0.5779\n",
      "Epoch [2200/4000], Loss:0.5761\n",
      "Epoch [2300/4000], Loss:0.5744\n",
      "Epoch [2400/4000], Loss:0.5728\n",
      "Epoch [2500/4000], Loss:0.5714\n",
      "Epoch [2600/4000], Loss:0.5700\n",
      "Epoch [2700/4000], Loss:0.5686\n",
      "Epoch [2800/4000], Loss:0.5674\n",
      "Epoch [2900/4000], Loss:0.5662\n",
      "Epoch [3000/4000], Loss:0.5651\n",
      "Epoch [3100/4000], Loss:0.5641\n",
      "Epoch [3200/4000], Loss:0.5631\n",
      "Epoch [3300/4000], Loss:0.5621\n",
      "Epoch [3400/4000], Loss:0.5612\n",
      "Epoch [3500/4000], Loss:0.5603\n",
      "Epoch [3600/4000], Loss:0.5595\n",
      "Epoch [3700/4000], Loss:0.5587\n",
      "Epoch [3800/4000], Loss:0.5580\n",
      "Epoch [3900/4000], Loss:0.5572\n",
      "----------------------\n",
      "test accuracy:  tensor(0.8100)\n"
     ]
    }
   ],
   "source": [
    "# 数据处理\n",
    "from torch.autograd import Variable\n",
    "_train_data = Variable(torch.from_numpy(train_data).float()) # 转换成可以反向传播的张量Variable\n",
    "_train_lab = Variable(torch.from_numpy(train_lab).long())\n",
    "_test_data = Variable(torch.from_numpy(test_data).float())\n",
    "_test_lab = Variable(torch.from_numpy(test_lab).long())\n",
    "\n",
    "# 模型定义\n",
    "class LR(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(LR, self).__init__()\n",
    "        self.func = nn.Linear(input_size, 2) #两个参数分别表示一条数据的输入特征数和分类器的类数\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        dataout = self.sigmoid(self.func(x))\n",
    "        return dataout\n",
    "    \n",
    "# 训练\n",
    "\n",
    "model = LR(train_data.shape[1])\n",
    "lossFunc = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = 0.01)\n",
    "\n",
    "epochs = 4000\n",
    "for epoch in range(epochs):\n",
    "    outs = model(_train_data)\n",
    "    loss = lossFunc(outs, _train_lab)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if epoch % 100 == 0:\n",
    "        print('Epoch [{}/{}], Loss:{:.4f}'.format(epoch, epochs, loss.item()))\n",
    "        \n",
    "# 测试\n",
    "testouts = model(_test_data)\n",
    "# print(testouts[:5])\n",
    "_, predicts = torch.max(testouts.data, 1)\n",
    "# print(predicts[:5])\n",
    "correct = (predicts == _test_lab).sum()\n",
    "print('----------------------')\n",
    "print('test accuracy: ', correct / test_data.shape[0])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 不使用torch库（自行编写代码）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:  0.71\n"
     ]
    }
   ],
   "source": [
    "# 简单逻辑回归\n",
    "def sigmoid(num):\n",
    "    return 1.0 / (1 + np.exp(-num))\n",
    "\n",
    "def grad_ascent(train_data, train_lab):\n",
    "    m, n =  np.shape(train_data)\n",
    "    w = np.ones(n)\n",
    "    alpha = 0.01\n",
    "    max_cycle = 200\n",
    "    for i in range(max_cycle):\n",
    "        s = sigmoid(np.dot(train_data, w))\n",
    "        error = train_lab - s\n",
    "        w += alpha * np.dot(train_data.transpose(), error)\n",
    "    return w\n",
    "\n",
    "w = grad_ascent(train_data, train_lab)\n",
    "predict_lab = np.round(sigmoid(np.dot(test_data, w)))\n",
    "fault_cnt = 0\n",
    "for i in range(np.shape(test_data)[0]):\n",
    "    if not predict_lab[i] == test_lab[i]:\n",
    "        fault_cnt += 1\n",
    "print(\"accuracy: \", 1 - fault_cnt / np.shape(test_data)[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:  0.73\n"
     ]
    }
   ],
   "source": [
    "# 逻辑回归 + 随机梯度下降\n",
    "def sigmoid(num):\n",
    "    return 1.0 / (1 + np.exp(-num))\n",
    "\n",
    "def grad_ascent(train_data, train_lab):\n",
    "    m, n =  np.shape(train_data)\n",
    "    w = np.ones(n)\n",
    "    # alpha = 0.01\n",
    "    max_cycle = 200\n",
    "    for i in range(max_cycle):\n",
    "        data_index = list(range(m))\n",
    "        for j in range(m // 2):\n",
    "            alpha = 1.0 / (1 + i + j) + 0.001\n",
    "            rand_index = int(np.random.uniform(0, len(data_index)))\n",
    "            data = train_data[data_index[rand_index]]\n",
    "            s = sigmoid(np.dot(data, w))\n",
    "            error = train_lab[data_index[rand_index]] - s\n",
    "            w += alpha * error * data\n",
    "            del (data_index[rand_index])\n",
    "    return w\n",
    "\n",
    "w = grad_ascent(train_data, train_lab)\n",
    "predict_lab = np.round(sigmoid(np.dot(test_data, w)))\n",
    "fault_cnt = 0\n",
    "for i in range(np.shape(test_data)[0]):\n",
    "    if not predict_lab[i] == test_lab[i]:\n",
    "        fault_cnt += 1\n",
    "print(\"accuracy: \", 1 - fault_cnt / np.shape(test_data)[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
